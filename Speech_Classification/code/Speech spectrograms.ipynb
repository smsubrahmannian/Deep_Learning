{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import librosa\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm,tqdm_notebook\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm,tnrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A_train = A[(A[' material']=='train')& (A[' corpus']=='Switchboard')].head(500)\n",
    "# A_train.reset_index(drop=True,inplace=True)\n",
    "# A_test = A[(A[' material']=='train')& (A[' corpus']=='Switchboard')].tail(100)\n",
    "# A_test[' material'] = 'dev'\n",
    "# A_test.reset_index(drop=True,inplace=True)\n",
    "# A_final = A_train.append(A_test)\n",
    "# A_final.reset_index(drop=True,inplace=True)\n",
    "# A_final.to_csv('./sample_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ssubrahmannian/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (7,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "A_final = pd.read_csv('/home/dgbrizan/project/CALS_tools/file_listing.csv')\n",
    "# A_final = A_final[(A_final[' material']=='train')& (A_final[' corpus']=='Switchboard')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_final = A_final[(A_final[' corpus']==' Switchboard')&(A_final[' material']==' train')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakerList = A_final[' speaker'].unique().tolist()\n",
    "speakerList_train = speakerList[:175]\n",
    "speakerList_dev = speakerList[175:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_final.loc[A_final[' speaker'].isin(speakerList_train),' material'] = 'train'\n",
    "A_final.loc[A_final[' speaker'].isin(speakerList_dev),' material'] = 'dev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " material   gender\n",
       "dev         female    18644\n",
       "            male      18804\n",
       "train       female    65872\n",
       "            male      48967\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_final.groupby([' material',' gender']).size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_final.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_final.to_csv('./switchboardData.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sample \n",
    "B = pd.read_csv('./switchboardData.csv')\n",
    "B_train = B[B[' material']=='train'].reset_index(drop=True)\n",
    "B_train_sample = B_train.groupby(' gender').apply(lambda x: x.sample(500)).reset_index(drop=True)\n",
    "B_dev = B[B[' material']=='dev'].reset_index(drop=True)\n",
    "B_dev_sample = B_dev.groupby(' gender').apply(lambda x: x.sample(200)).reset_index(drop=True)\n",
    "B_final_sample = B_train_sample.append(B_dev_sample,ignore_index=True)\n",
    "B_final_sample.to_csv('./switchboardDataSample.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class swbd_data(Dataset):\n",
    "    \n",
    "    def __init__(self,dataset,label,feat_len=40,paddedlen=660,encoder=None):\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.feat_len = feat_len\n",
    "        self.paddedlen = paddedlen\n",
    "        self.file_descriptions = None\n",
    "        self.__longest_vector__ = 0\n",
    "        self.encoder = encoder\n",
    "        self.instance_label = None\n",
    "        self.instance_list = []\n",
    "        self.label_column = label\n",
    "#         self._load_features_and_labels()\n",
    "        \n",
    "        \n",
    "    def _get_file_address(self):\n",
    "        \n",
    "        type_dict = {'file': 'O',\n",
    "             ' material': 'category',\n",
    "             ' speaker': 'category',\n",
    "             ' generation_description': 'category',\n",
    "             ' generation_decade': 'category',\n",
    "             ' gender': 'category',\n",
    "             ' region': 'category',\n",
    "             ' year': 'category',\n",
    "             ' L1': 'category',\n",
    "             ' corpus': 'category',\n",
    "             ' education': 'category'}\n",
    "#         filename = '/home/dgbrizan/project/CALS_tools/file_listing.csv'\n",
    "        filename = './switchboardDataSample.csv'\n",
    "        df = pd.read_csv(filename,dtype=type_dict)\n",
    "        df[df.columns] = df.apply(lambda x: x.str.strip())\n",
    "        final_df = df[(df[' material']==self.dataset)&(df[' corpus']=='Switchboard')].reset_index(drop=True)\n",
    "        return final_df\n",
    " \n",
    "\n",
    "    def __get_melfilter_bank_matrix__(self,file_name):\n",
    "        \n",
    "        # Parameters\n",
    "        n_fft   = 512             # FFT size of 512 \n",
    "        win_len = 512             # window size of 512 \n",
    "        hop_len = 160             # more than 50% overlap while doing FFT\n",
    "        n_mels  = self.feat_len   # applying 40 filters on Mel-scale to the power spectrum to extract frequency bands\n",
    "        htk     = True            # Use f = 700(10m/2595âˆ’1) to convert Hz to mel \n",
    "        upper_f = 4000\n",
    "        lower_f = 0\n",
    "        window  = sp.signal.hamming(n_fft, sym=True) # window function used to make the wav file suitable for STFT  \n",
    "    \n",
    "        wav, sr = librosa.load(file_name, sr=None, mono=True)                       # Step-1: load the wav file\n",
    "        magnitude_spectrogram = (1/n_fft)*np.abs(librosa.stft(wav, n_fft=n_fft, \n",
    "                                                              win_length=win_len,\n",
    "                                                              hop_length=hop_len, \n",
    "                                                              window=window)) ** 2  # Step-2: Create powerspectrum\n",
    "        mel_basis    = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=n_mels,\n",
    "                                        fmin=lower_f, fmax=upper_f, htk=htk)        # Step-3: Filters\n",
    "        mel_spectrum = np.dot(mel_basis, magnitude_spectrogram)                     # Step-4: get melspectrum\n",
    "        mel_spectrum = np.where(mel_spectrum== 0, np.finfo(float).eps, mel_spectrum)# Numerical Stability\n",
    "        mel_spectrum = 20 * np.log10(mel_spectrum)                                  # dB\n",
    "        mel_spectrum-= (np.mean(mel_spectrum, axis=0)) # mean normalization to improve Signal to Noise ratio\n",
    "        return mel_spectrum\n",
    " \n",
    "\n",
    "    def _load_features_and_labels(self):\n",
    "        \n",
    "        self.file_descriptions = self._get_file_address()\n",
    "        file_label = []\n",
    "        padded = []  \n",
    "        file_address_list = self.file_descriptions['file'].values\n",
    "        \n",
    "        for description in tqdm_notebook(file_address_list):\n",
    "            unpadded = self.__get_melfilter_bank_matrix__(description)\n",
    "            if len(unpadded[0]) > self.__longest_vector__:\n",
    "                self.__longest_vector__ = len(unpadded[0])\n",
    "            self.instance_list.append(unpadded)\n",
    "        \n",
    "        if self.paddedlen > self.__longest_vector__:\n",
    "            self.paddedlen = self.__longest_vector__\n",
    "            print('Final padded length truncated to longest vector')\n",
    "        \n",
    "        for i,instance in enumerate(self.instance_list):\n",
    "            if instance.shape[1]> self.paddedlen:\n",
    "                chopped_sample = self._chop_samples(instance,20)\n",
    "                rand_int  = np.random.randint(0,20)\n",
    "                instance = chopped_sample[rand_int]\n",
    "                \n",
    "            if instance.shape[1]<self.paddedlen:\n",
    "                instance = self._pad_samples(instance)\n",
    "                \n",
    "            padded.append(instance)\n",
    "            file_label.append(self.file_descriptions.loc[i,self.label_column])\n",
    "        \n",
    "        self.instance_list = padded\n",
    "        \n",
    "        if self.encoder == None:\n",
    "            self.encoder = LabelEncoder()\n",
    "            self.encoder.fit(file_label)\n",
    "            self.instance_label = self.encoder.transform(file_label).flatten()\n",
    "            return self.encoder\n",
    "        \n",
    "        else:\n",
    "            self.instance_label = self.encoder.transform(file_label).flatten()  \n",
    "        \n",
    "    \n",
    "    def _chop_samples(self,unaligned,num=20):\n",
    "        \n",
    "        padded_augments =[]\n",
    "        for i in range(num):\n",
    "            beg = np.random.randint(0, unaligned.shape[1] - self.paddedlen)\n",
    "            padded_augments.append(unaligned[:,beg: beg + self.paddedlen])\n",
    "        return padded_augments\n",
    "  \n",
    "\n",
    "    def _pad_samples(self,unpadded):\n",
    "    \n",
    "        if unpadded.shape[1]<self.paddedlen:\n",
    "            return np.pad(unpadded,pad_width=[(0,0),(0,self.paddedlen-unpadded.shape[1])],\n",
    "                          mode='constant',constant_values=0)\n",
    "\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        mel_spect = self.instance_list[index]\n",
    "        mel_spect = torch.Tensor(mel_spect).unsqueeze(0)\n",
    "    \n",
    "        return mel_spect, self.instance_label[index]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instance_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b076e58e21c41febb4d36f49a9e2eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_ds = swbd_data('train',' gender')\n",
    "encoder = train_ds._load_features_and_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33fedbc8dd024c76860cd9d86dbcf4f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=400), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_ds = swbd_data('dev',' gender',encoder =encoder)\n",
    "val_ds._load_features_and_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 40, 660])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size  = 138\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size,shuffle=True)\n",
    "vld_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([138, 1, 40, 660])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dl))[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_file_address():\n",
    "        \n",
    "        type_dict = {'file': 'O',\n",
    "             ' material': 'category',\n",
    "             ' speaker': 'category',\n",
    "             ' generation_description': 'category',\n",
    "             ' generation_decade': 'category',\n",
    "             ' gender': 'category',\n",
    "             ' region': 'category',\n",
    "             ' year': 'category',\n",
    "             ' L1': 'category',\n",
    "             ' corpus': 'category',\n",
    "             ' education': 'category'}\n",
    "        filename = '/home/dgbrizan/project/CALS_tools/file_listing.csv'\n",
    "        df = pd.read_csv(filename,dtype=type_dict,nrows=5000)\n",
    "        df[df.columns] = df.apply(lambda x: x.str.strip())\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "        self.batchnorm_11 = nn.BatchNorm2d(1)\n",
    "        self.conv2d_11 = nn.Conv2d(1,8,2)\n",
    "        self.conv2d_21 = nn.Conv2d(8,16,2)\n",
    "        self.mp1 = nn.MaxPool2d((2,2))\n",
    "        self.dp1 = nn.Dropout(p=0.2)   \n",
    "        \n",
    "        self.batchnrm_f1 = nn.BatchNorm1d(100016)\n",
    "        self.lin1 = nn.Linear(100016, 10000)\n",
    "        self.lin2 = nn.Linear(10000, 1000)\n",
    "        self.lin3 = nn.Linear(1000, 256)\n",
    "        self.lin4 = nn.Linear(256, 32)  \n",
    "        self.lin5 = nn.Linear(32,1)\n",
    "    def forward(self, spect):\n",
    "        x = F.relu(self.conv2d_11(spect))\n",
    "        x = F.relu(self.conv2d_21(x))\n",
    "        x = self.mp1(x)\n",
    "        x = self.dp1(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.batchnrm_f1(x)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.relu(self.lin3(x))\n",
    "        x = F.relu(self.lin4(x))\n",
    "        x = F.sigmoid(self.lin5(x))\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optim, train_dl=train_dl, verbose=False, unsqueeze =False,gpu=False):\n",
    "\n",
    "    # Setting model in train mode\n",
    "    model.train()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    for x_cat,price in tqdm(train_dl,desc='Training',dynamic_ncols=True):\n",
    "\n",
    "        batch = price.shape[0]\n",
    "        price = price.unsqueeze(1)\n",
    "        price = price.float()\n",
    "        if gpu:\n",
    "            x_cat = x_cat.cuda()\n",
    "            price = price.cuda()\n",
    "        y_hat = model(x_cat)\n",
    "        if unsqueeze: y_hat = y_hat.unsqueeze(1)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat,price)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total += batch\n",
    "        sum_loss += batch*(loss.item())\n",
    "        if verbose: print(np.sqrt(sum_loss/total))\n",
    "    return sum_loss/total\n",
    "\n",
    "def val_loss(model, valid_dl,unsqueeze=False,gpu= False):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    for x_cat,price in tqdm(valid_dl,desc='Validation',dynamic_ncols=True):\n",
    "        batch = price.shape[0]\n",
    "        price = price.unsqueeze(1)\n",
    "        price = price.float()\n",
    "        if gpu:\n",
    "            x_cat = x_cat.cuda()\n",
    "            price = price.cuda()\n",
    "\n",
    "        y_hat = model(x_cat)\n",
    "        if unsqueeze: y_hat = y_hat.unsqueeze(1)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat,price)\n",
    "        sum_loss += batch*(loss.item())\n",
    "        total += batch\n",
    "    return sum_loss/total\n",
    "\n",
    "OUTPUT_PATH = '/data/ssubrahmannian/DL/hw2/models/'\n",
    "def train_loop(model, epochs, optim, lr=0.01, wd=0.0,gpu=False):\n",
    "    for i in tnrange(epochs,desc='Epochs:',dynamic_ncols=True): \n",
    "        train_loss = train_model(model, optim, train_dl,gpu=gpu)\n",
    "        vld_loss = val_loss(model, valid_dl=vld_dl,gpu=gpu)\n",
    "        tqdm.write(\"Training Loss : %f 'Validation Loss: %f\"%(np.sqrt(train_loss),np.sqrt(vld_loss)),nolock=True)\n",
    "        if i%10 ==0: \n",
    "            model_output_path = f'{OUTPUT_PATH}model_{i}.pth'\n",
    "            save_model(model,model_output_path)\n",
    "\n",
    "def get_optimizer(model,lr,wd=0):\n",
    "    # Setting optimizer\n",
    "    parameters = filter(lambda x:x.requires_grad,model.parameters())\n",
    "    optim = torch.optim.Adam(parameters, lr=lr,weight_decay=wd)\n",
    "    return optim\n",
    "\n",
    "def save_model(m, p): torch.save(m.state_dict(), p)\n",
    "    \n",
    "def load_model(m, p): m.load_state_dict(torch.load(p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "parameters = filter(lambda x:x.requires_grad,model.parameters())\n",
    "optim = torch.optim.Adam(parameters, lr=0.01,weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bdde7b8fac2455c96e68cb5a3e3d892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epochs:', layout=Layout(flex='2'), max=10), HTML(value='')), â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [12:33<00:00, 94.15s/it] \n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:03<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSLE : 0.834441 'Validation RMSLE: 0.832555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [14:22<00:00, 107.86s/it]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:03<00:00,  1.07s/it]\n",
      "Training:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSLE : 0.832555 'Validation RMSLE: 0.832555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [09:54<05:56, 118.99s/it]"
     ]
    }
   ],
   "source": [
    "train_loop(model,10,optim,gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
